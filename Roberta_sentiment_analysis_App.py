# -*- coding: utf-8 -*-
"""Roberta sentiment Analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10L1VfVMZLa62qTFdUIOURELW194TjJ4e
"""

# Install required libraries
!pip install datasets transformers huggingface_hub -q

# Import key libraries and packages
import numpy as np
import os
import pandas as pd

from datasets import load_dataset, load_metric
from huggingface_hub import notebook_login
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, TrainingArguments, Trainer
from google.colab import files
from google.colab import drive

import nltk
nltk.download('punkt')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from wordcloud import WordCloud
from nltk.tokenize import word_tokenize
from collections import Counter

# For specifying the axes tick format
import matplotlib.ticker as mtick
import matplotlib.pyplot as plt

# Disable Weights & Biases
# Disable Weights & Biases
os.environ["WANDB_DISABLED"] = "true"

drive.mount('/content/drive')

# Load the datasets
# Load the datasets
Train=pd.read_csv("/content/drive/MyDrive/Tweets_data/Train.csv")
Test= pd.read_csv("/content/drive/MyDrive/Tweets_data/Test.csv")


# DATA CLEANING


train_df.head()

test_df.head()

train_df.isnull().sum()

test_df.isnull().sum()

# EXPLORATORY DATA ANALYSIS

"""DISTRIBUTION OF TWEETS"""
ax = (Train['label'].value_counts()*100.0 /len(Train))\
.plot.pie(autopct='%.1f%%', labels = ['Neutral', 'Positive', 'Negative'],figsize =(5,5), fontsize = 12 )
ax.yaxis.set_major_formatter(mtick.PercentFormatter())
ax.set_ylabel('',fontsize = 12)
ax.set_title('Distribution of tweet', fontsize = 12)

plt.show()


# Agreement Analysis:
ax = Train['agreement'].value_counts().plot(kind = 'bar',rot = 45, width = 0.3)
ax.set_ylabel('Agreement')
ax.set_title('Agreement Distribution')

plt.show()

# Text Length Analysis
# Calculate the length of each text
Train['text_length'] = Train['safe_text'].apply(lambda x: len(x))

# Plot the text length distribution
plt.hist(Train['text_length'], bins=20)
plt.xlabel('Text Length')
plt.ylabel('Count')
plt.title('Text Length Distribution')
plt.show()

# Word Frequency Analysis
# Combine all the texts into a single string
all_text = ' '.join(Train['safe_text'])

# Tokenize the text into individual words
tokens = word_tokenize(all_text)

# Count the frequency of each word
word_freq = Counter(tokens)

# Get the top 10 most used words
top_10_words = word_freq.most_common(10)

# Print the top 10 words and their frequencies
for word, freq in top_10_words:
    print(f"Word: {word}\tFrequency: {freq}")



# Combine all the texts into a single string
all_text = ' '.join(Train['safe_text'])

# Tokenize the text into individual words
tokens = word_tokenize(all_text)

# Count the frequency of each word
word_freq = Counter(tokens)

# Get the top 10 most used words and their frequencies
top_10_words = word_freq.most_common(10)

# Extract the words and frequencies for plotting
words = [word for word, freq in top_10_words]
frequencies = [freq for word, freq in top_10_words]

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(words, frequencies)
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Top 10 Most Used Words')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()



"""Fine-tuning the roberta model"""

Train, eval = train_test_split(Train, test_size=0.2, random_state=42, stratify= Train['label'])

print(f"new dataframe shapes: train is {Train.shape}, eval is {eval.shape}")

# Save splitted subsets
Train.to_csv("/content/drive/MyDrive/Tweets_data/Train.csv", index=False)
eval.to_csv("/content/drive/MyDrive/Tweets_data/Test.csv", index=False)

dataset = load_dataset('csv',
                        data_files={'Train': "/content/drive/MyDrive/Tweets_data/Train.csv",
                        'eval': "/content/drive/MyDrive/Tweets_data/Train.csv"}, encoding = "ISO-8859-1")

# Instantiate the tokenizer
tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment", num_labels=3)

## Function to transform labels
def transform_labels(label):

    label = label['label']
    num = 0
    if label == -1: #'Negative'
        num = 0
    elif label == 0: #'Neutral'
        num = 1
    elif label == 1: #'Positive'
        num = 2

    return {'labels': num}

## Function to tokenize data
def tokenize_data(example):
    return tokenizer(example['safe_text'], padding='max_length',truncation=True, max_length = 256)

# Tokenize the tweets
dataset = dataset.map(tokenize_data, batched=True)

# Transform	labels and limit the columns
remove_columns = ['label', 'safe_text', 'agreement']
dataset = dataset.map(transform_labels, remove_columns=remove_columns)

# Define training arguments
training_args = TrainingArguments(
    "covid_tweets_sentiment_analysis_model",
    num_train_epochs=10,
    load_best_model_at_end=True,
    evaluation_strategy="epoch",
    save_strategy="epoch"
    )
# Load the pretrained model
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment", num_labels=3)


# Define evaluation metrics
metric = load_metric("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Instantiate the training and evaluation sets
train_dataset = dataset["Train"].shuffle(seed=24)
eval_dataset = dataset["eval"].shuffle(seed=24)

#converting training data to PyTorch tensors to speed up training and adding padding:
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)


# Reinstantiate the trainer for evaluation
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Launch the final evaluation
trainer.evaluate()